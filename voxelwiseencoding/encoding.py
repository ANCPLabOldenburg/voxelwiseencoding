# AUTOGENERATED! DO NOT EDIT! File to edit: encoding.ipynb (unless otherwise specified).

__all__ = ['product_moment_corr', 'get_model_plus_scores', 'BlockMultiOutput']

# Cell
#export
import numpy as np
#from sklearn.metrics import r2_score
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge, RidgeCV, MultiTaskLassoCV, LinearRegression, SGDRegressor
#import warnings
import joblib
import copy
from joblib import Parallel, delayed
from sklearn.multioutput import MultiOutputRegressor, _fit_estimator
#from sklearn.utils import check_X_y, check_array
from sklearn.utils.validation import check_is_fitted
from sklearn.base import RegressorMixin
from sklearn.preprocessing import StandardScaler
from scipy.special import btdtr


def product_moment_corr(x, y):
    """ Product-moment correlation for two ndarrays x, y """
    x = StandardScaler().fit_transform(x)
    y = StandardScaler().fit_transform(y)
    n = x.shape[0]
    r = (1/(n-1)) * (x*y).sum(axis=0)
    # From scipy.stats.pearsonr:
    # As explained in the docstring, the p-value can be computed as
    #     p = 2*dist.cdf(-abs(r))
    # where dist is the beta distribution on [-1, 1] with shape parameters
    # a = b = n/2 - 1.  `special.btdtr` is the CDF for the beta distribution
    # on [0, 1].  To use it, we make the transformation  x = (r + 1)/2; the
    # shape parameters do not change.  Then -abs(r) used in `cdf(-abs(r))`
    # becomes x = (-abs(r) + 1)/2 = 0.5*(1 - abs(r)).  (r is cast to float64
    # to avoid a TypeError raised by btdtr when r is higher precision.)
    ab = n / 2 - 1
    prob = 2 * btdtr(ab, ab, 0.5 * (1 - abs(np.float64(r))))
    return r, prob


def get_model_plus_scores(X, y, estimator=None, cv=None, scorer=None,
                          voxel_selection=False, validate=True, 
                          run_start_indices=None, model_dump_path=None,
                          **kwargs):
    """ Returns multiple estimator trained in a cross-validation of
        the data and scores on the left-out folds.

    Parameters
        X : ndarray of shape (samples, features)
        y : ndarray of shape (samples, targets)
        estimator : None or estimator object that implements fit and predict.
                    If None, uses RidgeCV per default.
                    Can also be one of {'RidgeCV', 'Ridge', 'LinearRegression', 'MultiTaskLassoCV', 'SGDRegressor'}
                    to use the corresponding sklearn estimator. Arguments may be passed to the estimator via **kwargs.
        cv : int, None, 'leave-one-run-out', or a cross-validation object that
             implements a split method, default is None, optional.
             int specifies the number of cross-validation splits of a KFold cross validation.
             None defaults to a scikit-learn KFold cross-validation with default settings.
             'leave-one-run-out' specifies leave-one-run-out cross-validation.
             A scikit-learn-like cross-validation object needs to implement a
             split method for X and y.
        scorer : None or any scoring function that returns (score, pvalue), optional
                 default uses product moment correlation
        voxel_selection : bool, optional, default False
                          Whether to only use voxels with variance larger than zero.
                          This will set scores for these voxels to zero.
        validate : bool, optional, default True
                     Whether to validate the model via cross-validation
                     or to just train the estimator
                     if False, scores will be computed on the training set
        run_start_indices: list of int, optional, default None
                     Start index of each run which is used to group data into
                     cross-validation folds. If provided and cv is 'leave-one-run-out',
                     a leave-one-run-out cross-validation splitter will be used.
        model_dump_path: Path where to save dumps of model coefficients.
                         Default None, None means no saving
        **kwargs : additional arguments that will be passed to the estimator

    Returns
        tuple of n_splits estimators trained on training folds or single estimator
        if validation is False and scores for all concatenated out-of-fold predictions
    """
    #from sklearn.utils.estimator_checks import check_regressor_multioutput
    if scorer is None:
        scorer = product_moment_corr
    if cv is None:
        cv = KFold()
    elif isinstance(cv, int):
        cv = KFold(n_splits=cv)
    elif cv == 'leave-one-run-out':
        if run_start_indices is None:
            raise Exception('Missing run_start_indices: run_start_indices has'
                            +' to be defined when setting cv to "leave-one-run-out"')
        else:
            from leave_one_run_out_splitter import LeaveOneRunOutSplitter
            cv = LeaveOneRunOutSplitter(run_start_indices)
    elif not hasattr(cv, 'split'):
        raise Exception(f'Invalid cv parameter: {cv}. Has to be one of int, None,'
                        +'"leave-one-run-out", or cv object that implements a split'
                        +' method.')
    
    #models = []
    score_list = []
    pval_list = []
    bold_prediction = []
    train_indices = []
    test_indices = []
    if estimator is None or estimator == 'RidgeCV':
        estimator = RidgeCV(**kwargs)
    elif estimator == 'Ridge':
        estimator = Ridge(**kwargs)
    elif estimator == 'LinearRegression':
        estimator = LinearRegression(**kwargs)
    elif estimator == 'MultiTaskLassoCV':
        estimator = MultiTaskLassoCV(**kwargs)
    elif estimator == 'SGDRegressor':
        estimator = SGDRegressor(**kwargs)

    if voxel_selection:
        voxel_var = np.var(y, axis=0)
        y = y[:, voxel_var > 0.]
    if validate:
        cv_fold_idx = 0
        for train, test in cv.split(X, y):
#            models.append(copy.deepcopy(estimator).fit(X[train], y[train]))
#            bold_prediction.append(models[-1].predict(X[test]))
            model = copy.deepcopy(estimator).fit(X[train], y[train])
            bold_prediction.append(model.predict(X[test]))
            train_indices.append(train)
            test_indices.append(test)
            if voxel_selection:
                scores = np.zeros_like(voxel_var)
                pvals = np.zeros_like(voxel_var)
#                scores[voxel_var > 0.] =  scorer(y[test], models[-1].predict(X[test]))
                scores[voxel_var > 0.], pvals[voxel_var > 0.] = scorer(y[test], bold_prediction[-1])
            else:
#                scores = scorer(y[test], models[-1].predict(X[test]))
                scores, pvals = scorer(y[test], bold_prediction[-1])
            score_list.append(scores[:, None])
            pval_list.append(pvals[:, None])
            print('Saving '+model_dump_path.format(cv_fold_idx))
            joblib.dump(model, model_dump_path.format(cv_fold_idx))
            cv_fold_idx += 1
        score_list = np.concatenate(score_list, axis=-1)
        pval_list = np.concatenate(pval_list, axis=-1)
        # bold_prediction = np.concatenate(bold_prediction, axis=0)
    else:
#        models = estimator.fit(X, y)
        estimator.fit(X, y)
        score_list = scorer(y, estimator.predict(X))
    return score_list, bold_prediction, train_indices, test_indices, pval_list
    #return models, score_list, bold_prediction, train_indices, test_indices

# Cell

class BlockMultiOutput(MultiOutputRegressor, RegressorMixin):
    """Multi target regression with block-wise fit
    This strategy consists of splitting the targets in blocks and fitting one regressor per block.
    The estimator used needs to natively support multioutput.

    Parameters

        estimator : estimator object
            An estimator object implementing `fit` and `predict` and supporting multioutput.
        n_blocks : int, optional, default=10
            The number of blocks for the target variable.
            This is a split along *targets* (columns of the array), not observations (rows of the array).
        n_jobs : int, optional, default=1
            The number of jobs to run in parallel for `fit`. If -1,
            then the number of jobs is set to the number of cores.
            When individual estimators are fast to train or predict
            using `n_jobs>1` can result in slower performance due
            to the overhead of spawning processes.
    """

    def __init__(self, estimator, n_blocks=10, n_jobs=1):
        self.estimator = estimator
        self.n_blocks = n_blocks
        self.n_jobs = n_jobs

    def fit(self, X, y, sample_weight=None):
        """ Fit the model to data.
        Fit a separate model for each chunk of output.

        Parameters

            X : (sparse) array-like, shape (n_samples, n_features)
                Data.
            y : (sparse) array-like, shape (n_samples, n_outputs)
                Multi-output targets. An indicator matrix turns on multilabel
                estimation.
            sample_weight : array-like, shape = (n_samples) or None
                Sample weights. If None, then samples are equally weighted.
                Only supported if the underlying regressor supports sample
                weights.

        Returns

            self : object
                Returns self
        """
        if not hasattr(self.estimator, "fit"):
            raise ValueError("The base estimator should implement a fit method")

        if y.ndim == 1:
            raise ValueError("y must have at least two dimensions for "
                             "multi-output regression but has only one.")

        if (sample_weight is not None and
                not has_fit_parameter(self.estimator, 'sample_weight')):
            raise ValueError("Underlying estimator does not support"
                             " sample weights.")
        kfold = KFold(n_splits=self.n_blocks)
        smpl_X, smpl_y = np.zeros((y.shape[1],1)), np.zeros((y.shape[1],1))
        self.estimators_ = Parallel(n_jobs=self.n_jobs)(
            delayed(_fit_estimator)(
                self.estimator, X, y[:, block], sample_weight)
            for _, block in kfold.split(smpl_X, smpl_y))
        return self

    def partial_predict(self, X):
        """Predict multi-output variable using a model
         trained for each target variable block and yields predictions for each block as an iterator.

        Parameters

        X : (sparse) array-like, shape (n_samples, n_features)
            Data.

        Returns

        y : (sparse) array-like, shape (n_samples, n_outputs)
            Multi-output targets predicted across multiple predictors.
            Note: Separate models are generated for each predictor.
        """
        check_is_fitted(self, 'estimators_')
        if not hasattr(self.estimator, "predict"):
            raise ValueError("The base estimator should implement a predict method")

        X = check_array(X, accept_sparse=True)

        for estimator in self.estimators_:
            yield estimator.predict(X)

    def predict(self, X):
        """Predict multi-output variable using a model
         trained for each target variable block.

        Parameters

            X : (sparse) array-like, shape (n_samples, n_features)
                Data.

        Returns

            y : (sparse) array-like, shape (n_samples, n_outputs)
                Multi-output targets predicted across multiple predictors.
                Note: Separate models are generated for each predictor.
        """
        check_is_fitted(self, 'estimators_')
        if not hasattr(self.estimator, "predict"):
            raise ValueError("The base estimator should implement a predict method")

        X = check_array(X, accept_sparse=True)

        y = Parallel(n_jobs=self.n_jobs)(
            delayed(e.predict)(X)
            for e in self.estimators_)

        return np.hstack(y)

    def score(self, X, y):
        """Returns the correlation of the prediction with the target for each output.

        Parameters

            X : array-like, shape (n_samples, n_features)
                Test samples.
            y : array-like, shape (n_samples) or (n_samples, n_outputs)
                True values for X.

        Returns

            score : float
                Correlation of self.predict(X) wrt. y.
        """
        from sklearn.preprocessing import StandardScaler
        from itertools import izip
        kfold = KFold(n_splits=self.n_blocks)
        smpl_X, smpl_y = np.zeros((y.shape[1],1)), np.zeros((y.shape[1],1))
        scores = []
        for prediction, (_, block) in izip(self.partial_predict(X), kfold.split(smpl_X, smpl_y)):
            mx = StandardScaler().fit_transform(prediction).astype('float32')
            my = StandardScaler().fit_transform(y[:, block]).astype('float32')
            n = mx.shape[0]
            r = (1/(n-1))*(mx*my).sum(axis=0)
            scores.append(r)
        return np.concatenate(scores)
