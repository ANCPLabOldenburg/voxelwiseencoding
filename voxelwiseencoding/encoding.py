# AUTOGENERATED! DO NOT EDIT! File to edit: encoding.ipynb (unless otherwise specified).

__all__ = ['product_moment_corr', 'get_model_plus_scores']

# Cell
#export
import numpy as np
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold
from sklearn.linear_model import RidgeCV
import warnings
import copy

def product_moment_corr(x,y):
    '''Product-moment correlation for two ndarrays x, y'''
    from sklearn.preprocessing import StandardScaler
    x = StandardScaler().fit_transform(x)
    y = StandardScaler().fit_transform(y)
    n = x.shape[0]
    r = (1/(n-1))*(x*y).sum(axis=0)
    return r

# Cell

def get_model_plus_scores(X, y, estimator=None, cv=None, scorer=None,
                          voxel_selection=True, validate=True, **kwargs):
    '''Returns multiple estimator trained in a cross-validation on n_splits of the data and scores on the left-out folds

    Parameters

        X : ndarray of shape (samples, features)
        y : ndarray of shape (samples, targets)
        estimator : None or estimator object that implements fit and predict
                    if None, uses RidgeCV per default
        cv : int, None, or a cross-validation object that implements a split method, default is None, optional.
             int specifies the number of cross-validation splits of a KFold cross validation
             None defaults to a scikit-learn KFold cross-validation with default settings
             a scikit-learn-like cross-validation object needs to implement a split method for X and y
        scorer : None or any sci-kit learn compatible scoring function, optional
                 default uses product moment correlation
        voxel_selection : bool, optional, default True
                          Whether to only use voxels with variance larger than zero.
                          This will set scores for these voxels to zero.
        validate : bool, optional, default True
                     Whether to validate the model via cross-validation
                     or to just train the estimator
                     if False, scores will be computed on the training set
        kwargs : additional parameters that will be used to initialize RidgeCV if estimator is None
    Returns
        tuple of n_splits estimators trained on training folds or single estimator if validation is False
        and scores for all concatenated out-of-fold predictions'''
    from sklearn.utils.estimator_checks import check_regressor_multioutput
    if scorer is None:
        scorer = product_moment_corr
    if cv is None:
        cv = KFold()
    if isinstance(cv, int):
        cv = KFold(n_splits=cv)
    models = []
    score_list = []
    if estimator is None:
        estimator = RidgeCV(**kwargs)

    if voxel_selection:
        voxel_var = np.var(y, axis=0)
        y = y[:, voxel_var > 0.]
    if validate:
        for train, test in cv.split(X, y):
            models.append(copy.deepcopy(estimator).fit(X[train], y[train]))
            if voxel_selection:
                scores = np.zeros_like(voxel_var)
                scores[voxel_var > 0.] =  scorer(y[test], models[-1].predict(X[test]))
            else:
                scores = scorer(y[test], models[-1].predict(X[test]))
            score_list.append(scores[:, None])
        score_list = np.concatenate(score_list, axis=-1)
    else:
        models = estimator.fit(X, y)
        score_list = scorer(y, estimator.predict(X))
    return models, score_list