---

title: Voxel-wise encoding models for BIDS datasets with naturalistic stimuli


keywords: fastai
sidebar: home_sidebar

summary: "This BIDS App lets you train voxelwise-encoding models for continuous (naturalistic) stimuli provided as a BIDS-compliant continuous recording file."
description: "This BIDS App lets you train voxelwise-encoding models for continuous (naturalistic) stimuli provided as a BIDS-compliant continuous recording file."
nb_path: "index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2><p>If you are only interested in using the Python module for preprocessing fMRI, lagging the stimulus, and training encoding models without the BIDS app, you can install this library by running <code>python setup.py</code> or <code>pip install voxelwiseencoding</code>.
You can use the BIDS app either via Docker or directly by calling <a href="/voxelwiseencoding/process_bids.html#run.py"><code>run.py</code></a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Description">Description<a class="anchor-link" href="#Description"> </a></h2><p>This app allows you to train voxel-wise encoding models for a BIDS dataset with a BIDS-compliant stimulus representation. To specify parameters for the processing of the stimulus (e.g.lagging and offsetting relative to fMRI), you can specify parameters that are supplied to <a href="/voxelwiseencoding/preprocessing.html#make_X_Y"><code>make_X_Y</code></a> in the <a href="/voxelwiseencoding/preprocessing.html"><code>preprocessing</code></a> module as a JSON file. Similarly you can specify parameters to be supplied to <a href="/voxelwiseencoding/encoding.html#get_ridge_plus_scores"><code>get_ridge_plus_scores</code></a> in the <a href="/voxelwiseencoding/encoding.html"><code>encoding</code></a> module as a JSON as well.
Masking is done by default, by checking for masks in <code>output_dir/masks/</code> that are either named <code>sub-PARTICIPANT_LABEL_mask.nii.gz</code> (where PARTICIPANT_LABEL is the label provided by the user) or that are named <code>group_mask.nii.gz</code>. To disable masking call with the lag <code>--no-masking</code>.
Voxel-encoding models are trained in a cross-validation scheme: the parameter <code>n_splits</code> that is supplied to <a href="/voxelwiseencoding/encoding.html#get_ridge_plus_scores"><code>get_ridge_plus_scores</code></a> via a configuation JSON file determines the number of folds in the cross-validation. Each fold is left out once, while a model is trained (and hyperparameters are tuned) on the remaining folds - model validation is done by voxel-wise <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">product moment correlation</a> between the predicted and observed fMRI activity for the left-out fold and saved as a 4D nifti in the output folder (with one image per left-out fold).
Similarly, for each left-out fold, Ridge regression models (trained on the remaining folds) are saved as a pickle file in the output folder.</p>
<h3 id="Example">Example<a class="anchor-link" href="#Example"> </a></h3><p>We are going to use <a href="https://openneuro.org/datasets/ds002322/versions/1.0.4">this</a> dataset to demonstrate an example workflow using the Python package.</p>
<p>First we need to download the data and extract a stimulus representation:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>aws s3 sync --no-sign-request s3://openneuro.org/ds002322 ds002322-download/
<span class="kn">import</span> <span class="nn">json</span>
<span class="c1"># these are the parameters for extracting a Mel spectrogram</span>
<span class="c1"># for computational ease in this example we want 1 sec segments of 31 Mel frequencies with a max frequency of * KHz</span>
<span class="n">mel_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_mels&#39;</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span> <span class="s1">&#39;sr&#39;</span><span class="p">:</span> <span class="mi">16000</span><span class="p">,</span> <span class="s1">&#39;hop_length&#39;</span><span class="p">:</span> <span class="mi">16000</span><span class="p">,</span> <span class="s1">&#39;n_fft&#39;</span><span class="p">:</span> <span class="mi">16000</span><span class="p">,</span> <span class="s1">&#39;fmax&#39;</span><span class="p">:</span> <span class="mi">8000</span><span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;config.json&#39;</span><span class="p">,</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fl</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">mel_params</span><span class="p">,</span> <span class="n">fl</span><span class="p">)</span>

<span class="o">!</span>git clone https://github.com/mjboos/audio2bidsstim/
<span class="o">!</span>pip install -r audio2bidsstim/requirements.txt
<span class="o">!</span>python audio2bidsstim/wav_files_to_bids_tsv.py ds002322-download/stimuli/DownTheRabbitHoleFinal_mono_exp120_NR16_pad.wav -c config.json
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We then need to copy the extracted stimulus representation into the BIDS folder.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>cp DownTheRabbitHoleFinal_mono_exp120_NR16_pad.tsv.gz ds002322-download/derivatives/task-alice_stim.tsv.gz
<span class="o">!</span>cp DownTheRabbitHoleFinal_mono_exp120_NR16_pad.json ds002322-download/derivatives/sub-18/sub-18_task-alice_stim.json
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And, lastly, because for this dataset the derivatives folder is missing timing information for the BOLD files - we are only interested in the TR - we have to copy that as well.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>cp ds002322-download/sub-18/sub-18_task-alice_bold.json ds002322-download/derivatives/sub-18/sub-18_task-alice_bold.json 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are now ready to define some model parameters and train the encoding model.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">voxelwiseencoding.process_bids</span> <span class="k">import</span> <span class="n">run_model_for_subject</span>

<span class="c1"># these are the parameters used for preprocessing the BOLD fMRI files</span>
<span class="n">bold_prep_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;standardize&#39;</span><span class="p">:</span> <span class="s1">&#39;zscore&#39;</span><span class="p">,</span> <span class="s1">&#39;detrend&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

<span class="c1"># and for lagging the stimulus as well - we want to include 6 sec stimulus segments to predict fMRI</span>
<span class="n">lagging_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lag_time&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">}</span>

<span class="c1"># these are the parameters for sklearn&#39;s Ridge estimator</span>
<span class="n">ridge_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alphas&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="s1">&#39;n_splits&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;normalize&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="n">ridges</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">computed_mask</span> <span class="o">=</span> <span class="n">run_model_for_subject</span><span class="p">(</span><span class="s1">&#39;18&#39;</span><span class="p">,</span> <span class="s1">&#39;ds002322-download/derivatives&#39;</span><span class="p">,</span>
                                                      <span class="n">task</span><span class="o">=</span><span class="s1">&#39;alice&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="s1">&#39;epi&#39;</span><span class="p">,</span> <span class="n">bold_prep_kwargs</span><span class="o">=</span><span class="n">bold_prep_params</span><span class="p">,</span>
                                                      <span class="n">preprocess_kwargs</span><span class="o">=</span><span class="n">lagging_params</span><span class="p">,</span> <span class="n">encoding_kwargs</span><span class="o">=</span><span class="n">ridge_params</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Documentation">Documentation<a class="anchor-link" href="#Documentation"> </a></h2><p><a href="https://mjboos.github.io/voxelwiseencoding">See here</a> for further documentation about the Python package and consult the "Usage" section about the BIDS app/terminal usage.</p>
<h2 id="How-to-report-errors">How to report errors<a class="anchor-link" href="#How-to-report-errors"> </a></h2><p>If you encounter errors with this code or have any questions about its uage, please open an issue on the Github repository <a href="https://github.com/mjboos/voxelwiseencoding/">here</a>.</p>
<h2 id="Usage">Usage<a class="anchor-link" href="#Usage"> </a></h2><p>run.py /path/to/your/BIDS/dir /output/path --task your_task --ses session --skip_bids_validator --participant_label 01</p>
<p>This App has the following command line arguments:</p>
<pre>usage: run.py [-h]
              [--participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]]
              [--skip_bids_validator] [-d DESC] [-t TASK] [-s SES] [-v]
              [-r RECORDING] [--detrend] [--standardize STANDARDIZE]
              [--preprocessing-config PREPROCESSING_CONFIG]
              [--encoding-config ENCODING_CONFIG] [--identifier IDENTIFIER]
              [--no-masking] [--log]
              bids_dir output_dir

Voxelwise Encoding BIDS App.

positional arguments:
  bids_dir              The directory with the input dataset formatted
                        according to the BIDS standard.
  output_dir            The directory where the output files should be stored.
                        If you want to mask the data please include folder
                        called masks that contains either subject-specific
                        NifTI masks named sub-&lt;participant_label&gt;_mask.nii.gz
                        or a group-level mask named group_mask.nii.gz.

optional arguments:
  -h, --help            show this help message and exit
  --participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]
                        The label(s) of the participant(s) that should be
                        analyzed. The label corresponds to
                        sub-&lt;participant_label&gt; from the BIDS spec (so it does
                        not include &quot;sub-&quot;). If this parameter is not provided
                        all subjects should be analyzed. Multiple participants
                        can be specified with a space separated list.
  --skip_bids_validator
                        Whether or not to perform BIDS dataset validation
  -d DESC, --desc DESC  The label of the preprocessed data to use. Corresponds
                        to label in desc-&lt;label&gt; in the naming of the BOLD
                        NifTIs. If not provided, assumes no derivative label
                        is used.
  -t TASK, --task TASK  The task-label to use for training the voxel-wise
                        encoding model. Corresponds to label in task-&lt;label&gt;
                        in BIDS naming.
  -s SES, --ses SES     The label of the session to use. Corresponds to label
                        in ses-&lt;label&gt; in the BIDS directory.
  -v, --version         show program&apos;s version number and exit
  -r RECORDING, --recording RECORDING
                        The label of the stimulus recording to use.
                        Corresponds to label in recording-&lt;label&gt; of the
                        stimulus.
  --detrend             Whether to linearly detrend fMRI data voxel-wise
                        before training encoding models. Default is False.
  --standardize STANDARDIZE
                        How to voxel-wise standardize fMRI data before
                        training encoding models. Default is no
                        standardization, options are zscore for z-scoring and
                        psc for computing percent signal change.
  --preprocessing-config PREPROCESSING_CONFIG
                        Path to the preprocessing config file in JSON format.
                        Parameters in this file will be supplied as keyword
                        arguments to the make_X_Y function.
  --encoding-config ENCODING_CONFIG
                        Path to the encoding config file in JSON format.
                        Parameters in this file will be supplied as keyword
                        arguments to the get_ridge_plus_scores function.
  --identifier IDENTIFIER
                        Identifier to be included in the filenames for the
                        encoding model output.Use this to differentiate
                        different preprocessing steps or hyperparameters.
  --no-masking          Flag to disable masking. This will lead to many non-
                        brain voxels being included.
  --log                 Save preprocessing and model configuration together
                        with model output.
</pre>
</div>
</div>
</div>
</div>
 

