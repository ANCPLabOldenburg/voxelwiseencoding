{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "def product_moment_corr(x,y):\n",
    "    '''Product-moment correlation for two ndarrays x, y'''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    y = StandardScaler().fit_transform(y)\n",
    "    n = x.shape[0]\n",
    "    r = (1/(n-1))*(x*y).sum(axis=0)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating voxel-wise encoding models\n",
    "> Functions for training independent Ridge regressions for a large number of voxels and validating their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_model_plus_scores(X, y, estimator=None, n_splits=8, scorer=None,\n",
    "                          voxel_selection=True, validate=True, **kwargs):\n",
    "    '''Returns multiple estimator trained in a cross-validation on n_splits of the data and scores on the left-out folds\n",
    "\n",
    "    Parameters\n",
    "\n",
    "        X : ndarray of shape (samples, features)\n",
    "        y : ndarray of shape (samples, targets)\n",
    "        estimator : None or estimator object that implements fit and predict\n",
    "                    if None, uses RidgeCV per default\n",
    "        n_splits : int, optional, number of cross-validation splits\n",
    "        scorer : None or any sci-kit learn compatible scoring function, optional\n",
    "                 default uses product moment correlation\n",
    "        voxel_selection : bool, optional, default True\n",
    "                          Whether to only use voxels with variance larger than zero.\n",
    "                          This will set scores for these voxels to zero.\n",
    "        validate : bool, optional, default True\n",
    "                     Whether to validate the model via cross-validation\n",
    "                     or to just train the estimator\n",
    "                     if False, scores will be computed on the training set\n",
    "        kwargs : additional parameters that will be used to initialize RidgeCV if estimator is None \n",
    "    Returns\n",
    "        tuple of n_splits estimators trained on training folds or single estimator if validation is False\n",
    "        and scores for all concatenated out-of-fold predictions'''\n",
    "    from sklearn.utils.estimator_checks import check_regressor_multioutput\n",
    "    if scorer is None:\n",
    "        scorer = product_moment_corr\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    models = []\n",
    "    score_list = []\n",
    "    if estimator is None:\n",
    "        estimator = RidgeCV(**kwargs)\n",
    "        \n",
    "    if voxel_selection:\n",
    "        voxel_var = np.var(y, axis=0)\n",
    "        y = y[:, voxel_var > 0.]\n",
    "    if validate:\n",
    "        for train, test in kfold.split(X, y):\n",
    "            models.append(copy.deepcopy(estimator).fit(X[train], y[train]))\n",
    "            if voxel_selection:\n",
    "                scores = np.zeros_like(voxel_var)\n",
    "                scores[voxel_var > 0.] =  scorer(y[test], models[-1].predict(X[test]))\n",
    "            else:\n",
    "                scores = scorer(y[test], models[-1].predict(X[test]))\n",
    "            score_list.append(scores[:, None])\n",
    "        score_list = np.concatenate(score_list, axis=-1)\n",
    "    else:\n",
    "        models = estimator.fit(X, y)\n",
    "        score_list = scorer(y, estimator.predict(X))\n",
    "    return models, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_model_plus_scores` is a convenience function that trains `n_splits` Ridge regressions in a cross-validation scheme and evaluates their performance on the respective test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "First, we create some simulated `stimulus` and `fmri` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus = np.random.randn(1000, 5)\n",
    "fmri = np.random.randn(1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default Ridge regression\n",
    "\n",
    "We can now use `get_model_plus_scores` to estimate multiple [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) regressions, one for each voxel (that maps the stimulus representation to this voxel) and one for each split (trained on a different training set and evaluated on the held-out set).\n",
    "Since sklearn's `RidgeCV` estimator allows multi-output, we get one `RidgeCV` object per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RidgeCV(alphas=array([ 0.1,  1. , 10. ])),\n",
       " RidgeCV(alphas=array([ 0.1,  1. , 10. ])),\n",
       " RidgeCV(alphas=array([ 0.1,  1. , 10. ]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridges, scores = get_model_plus_scores(stimulus, fmri, n_splits=3)\n",
    "assert len(ridges) == 3\n",
    "ridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `RidgeCV` estimator maps from the feature space to each voxel.\n",
    "In our example, that means it has 10 (the number of voxels-9 independently trained regression models with 5 coeficients each (the number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01930266  0.0350985  -0.04548384 -0.01058159 -0.07382483]\n",
      " [ 0.00183012 -0.00830046 -0.03604675 -0.00016843  0.03161116]\n",
      " [-0.04032306  0.01782385  0.02112695  0.01673908 -0.00645515]\n",
      " [ 0.0273047  -0.02382577 -0.06169262  0.06232742 -0.03331368]\n",
      " [ 0.01294108 -0.04825337 -0.04646228 -0.04701512 -0.00017405]\n",
      " [ 0.02008884 -0.07065883  0.01958404 -0.04115758 -0.02967363]\n",
      " [ 0.00502653 -0.02164034 -0.00419562 -0.05675778  0.00716245]\n",
      " [ 0.0080379   0.03230623  0.01527909 -0.02469508 -0.01681562]\n",
      " [ 0.01363082  0.02686557 -0.05923971  0.01392573 -0.00945206]\n",
      " [ 0.01665226 -0.01499506 -0.0043113  -0.01658976  0.06103525]]\n"
     ]
    }
   ],
   "source": [
    "assert ridges[0].coef_.shape == (10, 5)\n",
    "print(ridges[0].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a set of scores (by default the [product moment correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), but you can supply your own via the `scorer` argument) that specifies how well we predict left-out data (with the usual caveats of using a correlation coefficient for evaluating it). In our case it is of shape (10, 3) because we predict 10 voxels and use a 3-fold cross-validation, i.e. we split 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11195214,  0.10260332,  0.02153565],\n",
       "       [-0.06450754, -0.07106461, -0.0447989 ],\n",
       "       [ 0.00559572, -0.03221425,  0.02866726],\n",
       "       [-0.04101258, -0.02197306, -0.04277958],\n",
       "       [ 0.02352969,  0.02008923, -0.02062713],\n",
       "       [ 0.01027339,  0.03074076,  0.01248573],\n",
       "       [-0.05974497, -0.03980094, -0.11293944],\n",
       "       [-0.01607721, -0.02264425, -0.07340733],\n",
       "       [-0.06009815, -0.05553956,  0.02102434],\n",
       "       [ 0.02388894, -0.01513094,  0.0904367 ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert scores.shape == (10, 3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the parameters of the `RidgeCV` function.\n",
    "For example, we can use pre-specified hyperparameters, like the values of the regularization parameter $\\alpha$ we want to perform a gridsearch over or whether we want to normalize features. If we want to use other parameters for the default `RidgeCV`, we can just pass the parameters as additional keyword arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7df959e35b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                        normalize=True, alpha_per_target=True)\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mridges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mridges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphas = [100]\n",
    "ridges, scores = get_model_plus_scores(stimulus, fmri, n_splits=3, alphas=alphas,\n",
    "                                       normalize=True, alpha_per_target=True)\n",
    "assert ridges[0].normalize\n",
    "assert ridges[0].alphas.shape == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own estimator\n",
    "\n",
    "\n",
    "Additionally, we can use any other estimator that implements `fit` and `predict`.\n",
    "For example, we can use [CCA](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) as an encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_decomposition\n",
    "\n",
    "our_estimator = cross_decomposition.CCA(n_components=2)\n",
    "\n",
    "ccas, scores = get_model_plus_scores(stimulus, fmri, our_estimator,\n",
    "                                       n_splits=3)\n",
    "assert type(ccas[0]) == cross_decomposition._pls.CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your favorite estimator does not work in the multioutput regime, i.e. it cannot predict multiple targets/voxels, then `get_model_plus_scores` will wrap it into sklearn's [MultiOutputRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html) by default. However, for many voxels this can increase training time by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultiOutputRegressor(estimator=Lasso()),\n",
       " MultiOutputRegressor(estimator=Lasso()),\n",
       " MultiOutputRegressor(estimator=Lasso())]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "our_estimator = MultiOutputRegressor(Lasso())\n",
    "\n",
    "lassos, scores = get_model_plus_scores(stimulus, fmri, our_estimator,\n",
    "                                       n_splits=3)\n",
    "lassos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without validation\n",
    "\n",
    "We can also train an estimator without any validation, if, for example we want to test on a different dataset. In that case, the scores will be computed with the trained estimator on the training set, i.e. they will contain no information about the generalization performance of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_estimator = RidgeCV()\n",
    "\n",
    "model, scores = get_model_plus_scores(stimulus, fmri, our_estimator,\n",
    "                                       validation=False)\n",
    "assert type(model) == RidgeCV\n",
    "assert scores.shape == (10,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mne)",
   "language": "python",
   "name": "mne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
