{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def product_moment_corr(x,y):\n",
    "    '''Product-moment correlation for two ndarrays x, y'''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    y = StandardScaler().fit_transform(y)\n",
    "    n = x.shape[0]\n",
    "    r = (1/(n-1))*(x*y).sum(axis=0)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating voxel-wise encoding models\n",
    "> Functions for training independent Ridge regressions for a large number of voxels and validating their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ridge_plus_scores(X, y, alphas=None, n_splits=8, scorer=None, voxel_selection=True, **kwargs):\n",
    "    '''Returns ridge regressions trained in a cross-validation on n_splits of the data and scores on the left-out folds\n",
    "\n",
    "    Parameters\n",
    "\n",
    "        X : ndarray of shape (samples, features)\n",
    "        y : ndarray of shape (samples, targets)\n",
    "        alphas : None or list of floats, optional\n",
    "                 Regularization parameters to be used for Ridge regression\n",
    "        n_splits : int, optional\n",
    "        scorer : None or any sci-kit learn compatible scoring function, optional\n",
    "                 default uses product moment correlation\n",
    "        voxel_selection : bool, optional, default True\n",
    "                          Whether to only use voxels with variance larger than zero.\n",
    "                          This will set scores for these voxels to zero.\n",
    "        kwargs : additional arguments transferred to ridge_gridsearch_per_target\n",
    "\n",
    "    Returns\n",
    "        tuple of n_splits Ridge estimators trained on training folds\n",
    "        and scores for all concatenated out-of-fold predictions'''\n",
    "    if scorer is None:\n",
    "        scorer = product_moment_corr\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    if alphas is None:\n",
    "        alphas = [1000]\n",
    "    ridges = []\n",
    "    score_list = []\n",
    "    if voxel_selection:\n",
    "        voxel_var = np.var(y, axis=0)\n",
    "        y = y[:, voxel_var > 0.]\n",
    "    for train, test in kfold.split(X, y):\n",
    "        ridges.append(ridge_gridsearch_per_target(X[train], y[train], alphas, **kwargs))\n",
    "        if voxel_selection:\n",
    "            scores = np.zeros_like(voxel_var)\n",
    "            scores[voxel_var > 0.] =  scorer(y[test], ridges[-1].predict(X[test]))\n",
    "        else:\n",
    "            scores = scorer(y[test], ridges[-1].predict(X[test]))\n",
    "        score_list.append(scores[:, None])\n",
    "    return ridges, np.concatenate(score_list, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_ridge_plus_scores` is a convenience function that trains `n_splits` Ridge regressions in a cross-validation scheme and evaluates their performance on the respective test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ridge_gridsearch_per_target(X, y, alphas, n_splits=5, **kwargs):\n",
    "    '''Runs Ridge gridsearch across alphas for each target in y\n",
    "\n",
    "    Parameters\n",
    "\n",
    "        X : ndarray of shape (samples, features)\n",
    "        y : ndarray of shape (samples, targets)\n",
    "        alphas : None or list of floats, optional\n",
    "                 Regularization parameters to be used for Ridge regression\n",
    "        n_splits : int, optional\n",
    "        kwargs : keyword parameters to be transferred to Ridge regression\n",
    "\n",
    "    Returns\n",
    "        Ridge regression trained on X, y with optimal alpha per target\n",
    "        determined by KFold cross-validation\n",
    "    '''\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    cv_results = {'alphas': []}\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    for alpha in alphas:\n",
    "        scores = []\n",
    "        for train, test in cv.split(X, y):\n",
    "            ridge = Ridge(alpha=alpha, **kwargs)\n",
    "            scores.append(mean_squared_error(y[test], ridge.fit(X[train], y[train]).predict(X[test]),\n",
    "                              multioutput='raw_values'))\n",
    "        scores = np.vstack(scores).mean(axis=0)\n",
    "        cv_results['alphas'].append(scores)\n",
    "    cv_results['alphas'] = np.vstack(cv_results['alphas'])\n",
    "    best_alphas = np.array(alphas)[np.argmin(cv_results['alphas'], axis=0)]\n",
    "    return Ridge(alpha=best_alphas, **kwargs).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "First, we create some simulated `stimulus` and `fmri` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus = np.random.randn(1000, 5)\n",
    "fmri = np.random.randn(1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `get_ridge_plus_scores` to estimate multiple [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) regressions, one for each voxel (that maps the stimulus representation to this voxel) and one for each split (trained on a different training set and evaluated on the held-out set).\n",
    "Since sklearn's `Ridge` estimator allows multi-output, we get one `Ridge` object per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Ridge(alpha=array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000])),\n",
       " Ridge(alpha=array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000])),\n",
       " Ridge(alpha=array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridges, scores = get_ridge_plus_scores(stimulus, fmri, n_splits=3)\n",
    "print(len(ridges))\n",
    "ridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Ridge` estimator maps from the feature space to each voxel.\n",
    "In our example, that means it has 10 (the number of voxels-9 independently trained regression models with 5 coeficients each (the number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03091656 -0.00552985  0.01743601  0.01279568  0.01053411]\n",
      " [ 0.00238455  0.0045204   0.00256562 -0.00748261 -0.00547277]\n",
      " [-0.00422868 -0.02666707  0.01187212 -0.02378444  0.03065698]\n",
      " [ 0.03035251 -0.01177954  0.01668209 -0.0109779   0.01566495]\n",
      " [ 0.0076802  -0.01518692 -0.01891325 -0.00329026 -0.00599254]\n",
      " [-0.01751964  0.00928253  0.00283076  0.01285903 -0.02664727]\n",
      " [ 0.03044979  0.02130243  0.01450193 -0.00512366  0.0111223 ]\n",
      " [-0.00812324 -0.00502767 -0.00471382  0.00727634 -0.02175445]\n",
      " [ 0.03337194  0.01006145 -0.01272059 -0.01134752 -0.01544005]\n",
      " [ 0.00368844  0.01035971 -0.03002989 -0.01830897 -0.00994489]]\n"
     ]
    }
   ],
   "source": [
    "assert ridges[0].coef_.shape == (10, 5)\n",
    "print(ridges[0].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a set of scores (by default the [product moment correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), but you can supply your own via the `scorer` argument) that specifies how well we predict left-out data (with the usual caveats of using a correlation coefficient for evaluating it). In our case it is of shape (10, 3) because we predict 10 voxels and use a 3-fold cross-validation, i.e. we split 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01731239, -0.03907288,  0.04493863],\n",
       "       [ 0.00467211, -0.06636354, -0.0427261 ],\n",
       "       [-0.02978713,  0.06524192,  0.06223558],\n",
       "       [-0.03216798, -0.00690511, -0.02197701],\n",
       "       [ 0.07491743,  0.06247747, -0.00317411],\n",
       "       [-0.0297277 , -0.0455451 , -0.01514241],\n",
       "       [ 0.09316656,  0.0493194 ,  0.0818891 ],\n",
       "       [-0.01266644, -0.06847899, -0.06770772],\n",
       "       [ 0.04326812,  0.08139216,  0.04508078],\n",
       "       [ 0.06647038,  0.0544588 ,  0.02366174]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert scores.shape == (10, 3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also call specify which hyperparameters we want to use.\n",
    "For example the values of the regularization parameter $\\alpha$ we want to perform a gridsearch over or whether we want to normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1, 10, 100]\n",
    "ridges, scores = get_ridge_plus_scores(stimulus, fmri, n_splits=3,\n",
    "                                       alphas=alphas,\n",
    "                                       normalize=True)\n",
    "assert ridges[0].normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mne)",
   "language": "python",
   "name": "mne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
