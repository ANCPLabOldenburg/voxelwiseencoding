{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel-wise encoding models for BIDS datasets with naturalistic stimuli\n",
    "\n",
    "> This BIDS App lets you train voxelwise-encoding models for continuous (naturalistic) stimuli provided as a BIDS-compliant continuous recording file.\n",
    "\n",
    "![Python package](https://github.com/mjboos/voxelwiseencoding/workflows/Python%20package/badge.svg)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "For more information about the specification of BIDS Apps see [here](https://docs.google.com/document/d/1E1Wi5ONvOVVnGhj21S1bmJJ4kyHFT7tkxnV3C23sjIE/).\n",
    "For auditory stimuli [this](https://github.com/mjboos/audio2bidsstim/) module can help you convert your wav file to a BIDS stimulus representation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "If you are only interested in using the Python module for preprocessing fMRI, lagging the stimulus, and training encoding models without the BIDS app, you can install this library by running `python setup.py` or `pip install voxelwiseencoding`.\n",
    "You can use the BIDS app either via Docker or directly by calling `run.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This app allows you to train voxel-wise encoding models for a BIDS dataset with a BIDS-compliant stimulus representation. To specify parameters for the processing of the stimulus (e.g.lagging and offsetting relative to fMRI), you can specify parameters that are supplied to `make_X_Y` in the `preprocessing` module as a JSON file. Similarly you can specify parameters to be supplied to `get_ridge_plus_scores` in the `encoding` module as a JSON as well.\n",
    "Masking is done by default, by checking for masks in `output_dir/masks/` that are either named `sub-PARTICIPANT_LABEL_mask.nii.gz` (where PARTICIPANT_LABEL is the label provided by the user) or that are named `group_mask.nii.gz`. To disable masking call with the lag `--no-masking`.\n",
    "Voxel-encoding models are trained in a cross-validation scheme: the parameter `n_splits` that is supplied to `get_ridge_plus_scores` via a configuation JSON file determines the number of folds in the cross-validation. Each fold is left out once, while a model is trained (and hyperparameters are tuned) on the remaining folds - model validation is done by voxel-wise [product moment correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the predicted and observed fMRI activity for the left-out fold and saved as a 4D nifti in the output folder (with one image per left-out fold).\n",
    "Similarly, for each left-out fold, Ridge regression models (trained on the remaining folds) are saved as a pickle file in the output folder.\n",
    "\n",
    "### Example\n",
    "\n",
    "We are going to use [this](https://openneuro.org/datasets/ds002322/versions/1.0.4) dataset to demonstrate an example workflow using the Python package.\n",
    "\n",
    "First we need to download the data and extract a stimulus representation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --no-sign-request s3://openneuro.org/ds002322 ds002322-download/\n",
    "import json\n",
    "# these are the parameters for extracting a Mel spectrogram\n",
    "# for computational ease in this example we want 1 sec segments of 31 Mel frequencies with a max frequency of * KHz\n",
    "mel_params = {'n_mels': 31, 'sr': 16000, 'hop_length': 16000, 'n_fft': 16000, 'fmax': 8000}\n",
    "with open('config.json', 'w+') as fl:\n",
    "    json.dump(mel_params, fl)\n",
    "\n",
    "!git clone https://github.com/mjboos/audio2bidsstim/\n",
    "!pip install -r audio2bidsstim/requirements.txt\n",
    "!python audio2bidsstim/wav_files_to_bids_tsv.py ds002322-download/stimuli/DownTheRabbitHoleFinal_mono_exp120_NR16_pad.wav -c config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to copy the extracted stimulus representation into the BIDS folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp DownTheRabbitHoleFinal_mono_exp120_NR16_pad.tsv.gz ds002322-download/derivatives/task-alice_stim.tsv.gz\n",
    "!cp DownTheRabbitHoleFinal_mono_exp120_NR16_pad.json ds002322-download/derivatives/sub-18/sub-18_task-alice_stim.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, lastly, because for this dataset the derivatives folder is missing timing information for the BOLD files - we are only interested in the TR - we have to copy that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ds002322-download/sub-18/sub-18_task-alice_bold.json ds002322-download/derivatives/sub-18/sub-18_task-alice_bold.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define some model parameters and train the encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxelwiseencoding.process_bids import run_model_for_subject\n",
    "\n",
    "# these are the parameters used for preprocessing the BOLD fMRI files\n",
    "bold_prep_params = {'standardize': 'zscore', 'detrend': True}\n",
    "\n",
    "# and for lagging the stimulus as well - we want to include 6 sec stimulus segments to predict fMRI\n",
    "lagging_params = {'lag_time': 6}\n",
    "\n",
    "# these are the parameters for sklearn's Ridge estimator\n",
    "ridge_params = {'alphas': [1e-1, 1, 100, 1000], 'n_splits': 3, 'normalize': True}\n",
    "\n",
    "\n",
    "ridges, scores, computed_mask = run_model_for_subject('18', 'ds002322-download/derivatives',\n",
    "                                                      task='alice', mask='epi', bold_prep_kwargs=bold_prep_params,\n",
    "                                                      preprocess_kwargs=lagging_params, encoding_kwargs=ridge_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Documentation\n",
    "\n",
    "[See here](https://mjboos.github.io/voxelwiseencoding) for further documentation about the Python package and consult the \"Usage\" section about the BIDS app/terminal usage.\n",
    "\n",
    "## How to report errors\n",
    "\n",
    "If you encounter errors with this code or have any questions about its uage, please open an issue on the Github repository [here](https://github.com/mjboos/voxelwiseencoding/).\n",
    "\n",
    "## Usage\n",
    "\n",
    "run.py /path/to/your/BIDS/dir /output/path --task your_task --ses session --skip_bids_validator --participant_label 01\n",
    "\n",
    "This App has the following command line arguments:\n",
    "<pre>usage: run.py [-h]\n",
    "              [--participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]]\n",
    "              [--skip_bids_validator] [-d DESC] [-t TASK] [-s SES] [-v]\n",
    "              [-r RECORDING] [--detrend] [--standardize STANDARDIZE]\n",
    "              [--preprocessing-config PREPROCESSING_CONFIG]\n",
    "              [--encoding-config ENCODING_CONFIG] [--identifier IDENTIFIER]\n",
    "              [--no-masking] [--log]\n",
    "              bids_dir output_dir\n",
    "\n",
    "Voxelwise Encoding BIDS App.\n",
    "\n",
    "positional arguments:\n",
    "  bids_dir              The directory with the input dataset formatted\n",
    "                        according to the BIDS standard.\n",
    "  output_dir            The directory where the output files should be stored.\n",
    "                        If you want to mask the data please include folder\n",
    "                        called masks that contains either subject-specific\n",
    "                        NifTI masks named sub-&lt;participant_label&gt;_mask.nii.gz\n",
    "                        or a group-level mask named group_mask.nii.gz.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]\n",
    "                        The label(s) of the participant(s) that should be\n",
    "                        analyzed. The label corresponds to\n",
    "                        sub-&lt;participant_label&gt; from the BIDS spec (so it does\n",
    "                        not include &quot;sub-&quot;). If this parameter is not provided\n",
    "                        all subjects should be analyzed. Multiple participants\n",
    "                        can be specified with a space separated list.\n",
    "  --skip_bids_validator\n",
    "                        Whether or not to perform BIDS dataset validation\n",
    "  -d DESC, --desc DESC  The label of the preprocessed data to use. Corresponds\n",
    "                        to label in desc-&lt;label&gt; in the naming of the BOLD\n",
    "                        NifTIs. If not provided, assumes no derivative label\n",
    "                        is used.\n",
    "  -t TASK, --task TASK  The task-label to use for training the voxel-wise\n",
    "                        encoding model. Corresponds to label in task-&lt;label&gt;\n",
    "                        in BIDS naming.\n",
    "  -s SES, --ses SES     The label of the session to use. Corresponds to label\n",
    "                        in ses-&lt;label&gt; in the BIDS directory.\n",
    "  -v, --version         show program&apos;s version number and exit\n",
    "  -r RECORDING, --recording RECORDING\n",
    "                        The label of the stimulus recording to use.\n",
    "                        Corresponds to label in recording-&lt;label&gt; of the\n",
    "                        stimulus.\n",
    "  --detrend             Whether to linearly detrend fMRI data voxel-wise\n",
    "                        before training encoding models. Default is False.\n",
    "  --standardize STANDARDIZE\n",
    "                        How to voxel-wise standardize fMRI data before\n",
    "                        training encoding models. Default is no\n",
    "                        standardization, options are zscore for z-scoring and\n",
    "                        psc for computing percent signal change.\n",
    "  --preprocessing-config PREPROCESSING_CONFIG\n",
    "                        Path to the preprocessing config file in JSON format.\n",
    "                        Parameters in this file will be supplied as keyword\n",
    "                        arguments to the make_X_Y function.\n",
    "  --encoding-config ENCODING_CONFIG\n",
    "                        Path to the encoding config file in JSON format.\n",
    "                        Parameters in this file will be supplied as keyword\n",
    "                        arguments to the get_ridge_plus_scores function.\n",
    "  --identifier IDENTIFIER\n",
    "                        Identifier to be included in the filenames for the\n",
    "                        encoding model output.Use this to differentiate\n",
    "                        different preprocessing steps or hyperparameters.\n",
    "  --no-masking          Flag to disable masking. This will lead to many non-\n",
    "                        brain voxels being included.\n",
    "  --log                 Save preprocessing and model configuration together\n",
    "                        with model output.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mne)",
   "language": "python",
   "name": "mne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
